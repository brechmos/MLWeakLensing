{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements.txt file for virtualenv\n",
    "\n",
    "* Keras\n",
    "* numpy\n",
    "* astropy\n",
    "* jupyter\n",
    "* scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# convlulation layres to help train on image data\n",
    "from keras.layers import Conv2D, MaxPooling2D, SeparableConv2D\n",
    "\n",
    "# optimizers\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# core layers\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "# \n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "\n",
    "def rebin(a, shape):\n",
    "    sh = shape[0],a.shape[0]//shape[0],shape[1],a.shape[1]//shape[1]\n",
    "    return a.reshape(sh).mean(-1).mean(1)\n",
    "\n",
    "def blockshaped(arr, nrows, ncols):\n",
    "    \"\"\"\n",
    "    Return an array of shape (n, nrows, ncols) where\n",
    "    n * nrows * ncols = arr.size\n",
    "\n",
    "    If arr is a 2D array, the returned array should look like n subblocks with\n",
    "    each subblock preserving the \"physical\" layout of arr.\n",
    "    \"\"\"\n",
    "    h, w = arr.shape\n",
    "    return (arr.reshape(h//nrows, nrows, -1, ncols)\n",
    "               .swapaxes(1,2)\n",
    "               .reshape(-1, nrows, ncols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display = True\n",
    "labels = ['750', '850']\n",
    "path='/Users/crjones/Documents/Science/HargisDDRF/astroNN/data/wl_maps'\n",
    "degrade=8\n",
    "nct = 9\n",
    "imsize=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0001r_0029p_0100z_og.gre.fit\n",
      "i: 1  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0002r_0029p_0100z_og.gre.fit\n",
      "i: 2  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0003r_0029p_0100z_og.gre.fit\n",
      "i: 3  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0004r_0029p_0100z_og.gre.fit\n",
      "i: 4  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0005r_0029p_0100z_og.gre.fit\n",
      "i: 5  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0006r_0029p_0100z_og.gre.fit\n",
      "i: 6  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0007r_0029p_0100z_og.gre.fit\n",
      "i: 7  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0008r_0029p_0100z_og.gre.fit\n",
      "i: 8  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0009r_0029p_0100z_og.gre.fit\n",
      "i: 0  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0001r_0029p_0100z_og.gre.fit\n",
      "i: 1  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0002r_0029p_0100z_og.gre.fit\n",
      "i: 2  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0003r_0029p_0100z_og.gre.fit\n",
      "i: 3  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0004r_0029p_0100z_og.gre.fit\n",
      "i: 4  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0005r_0029p_0100z_og.gre.fit\n",
      "i: 5  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0006r_0029p_0100z_og.gre.fit\n",
      "i: 6  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0007r_0029p_0100z_og.gre.fit\n",
      "i: 7  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0008r_0029p_0100z_og.gre.fit\n",
      "i: 8  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0009r_0029p_0100z_og.gre.fit\n"
     ]
    }
   ],
   "source": [
    "imgs = np.zeros([2048//degrade, 2048//degrade, nct, len(labels)])\n",
    "imgs2 = {'750': [], '850': []}\n",
    "for j, label in enumerate(labels):\n",
    "    for i in range(nct):\n",
    "        filename = os.path.join(path, 'smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.'+label+'_4096xy_000'+ np.str(i+1) +'r_0029p_0100z_og.gre.fit')\n",
    "        if display:\n",
    "           print(\"i: %d  j: %d  name: %s\" % (i, j, 'smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.'+label+'_4096xy_000'+ np.str(i+1) +'r_0029p_0100z_og.gre.fit'))\n",
    "\n",
    "        # Read in the data and put into the imgs.\n",
    "        f = fits.open(filename)\n",
    "        imgs[:,:,i,j]=rebin(f[0].data, [2048//degrade, 2048//degrade])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotate, transpose the data to create new sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create the 8x8 sub images from the image just read in\n",
    "subimgs = {'750': [], '850': []}\n",
    "for j, label in enumerate(labels):\n",
    "    for i in range(nct):\n",
    "\n",
    "        # Grab the list of 8x8 sub arrays\n",
    "        temp_subimgs = [x for x in blockshaped(imgs[:,:,i,j], imsize, imsize)]\n",
    "\n",
    "        # Now create all the flips, rotations, transpositions etc\n",
    "        subimgs[label].extend(temp_subimgs)  # sub images\n",
    "        subimgs[label].extend([x.T for x in temp_subimgs]) # transposed sub images\n",
    "        subimgs[label].extend([np.rot90(x) for x in temp_subimgs]) # rotated sub images\n",
    "        subimgs[label].extend([np.rot90(x, k=2) for x in temp_subimgs]) # rotated twice sub images\n",
    "        subimgs[label].extend([np.rot90(x, k=3) for x in temp_subimgs]) # rotated three x sub images\n",
    "        subimgs[label].extend([x[::-1] for x in temp_subimgs]) # flip ud\n",
    "        subimgs[label].extend([x[:,::-1] for x in temp_subimgs]) # flip lr\n",
    "        \n",
    "        # Texture images\n",
    "        for im in temp_subimgs:\n",
    "            glcm = greycomatrix(((im-im.min())/(im.max()-im.min())*100).astype(int),  range(32), np.arange(0,2*np.pi,2*np.pi/32), 256, symmetric=True, normed=True)\n",
    "            cont = greycoprops(glcm, 'contrast')\n",
    "            diss = greycoprops(glcm, 'dissimilarity')\n",
    "            homo = greycoprops(glcm, 'homogeneity')\n",
    "            eng = greycoprops(glcm, 'energy')\n",
    "            corr = greycoprops(glcm, 'correlation')\n",
    "            ASM = greycoprops(glcm, 'ASM')\n",
    "            subimgs[label].extend([cont, diss, homo, eng, corr, ASM])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_training = int(0.9 * len(subimgs[label]) * 2)\n",
    "\n",
    "# Create the X_train and y_train from the '750' data\n",
    "training_random_inds_750 = np.random.choice(range(len(subimgs['750'])), N_training//2)\n",
    "X_train = [subimgs['750'][x]  for x in training_random_inds_750]\n",
    "y_train = [750]*(N_training//2)\n",
    "\n",
    "# Extend the X_train and y_train with the '850' data\n",
    "training_random_inds_850 = np.random.choice(range(len(subimgs['850'])), N_training//2)\n",
    "X_train.extend([subimgs['850'][x]  for x in training_random_inds_850])\n",
    "y_train.extend([850]*(N_training//2))\n",
    "\n",
    "# Randomize the list of training data\n",
    "inds = [x for x in range(len(X_train))]\n",
    "random.shuffle(inds)\n",
    "X_train = [X_train[ii] for ii in inds]\n",
    "y_train = [y_train[ii] for ii in inds]\n",
    "\n",
    "# Now make them into a 3D array and 1D array\n",
    "X_train = np.stack(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Add dimension to X_train for keras\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, imsize, imsize)\n",
    "\n",
    "# change y_train to labels of 0 and 1 (conversion of boolean to int)\n",
    "y_train = np.array(y_train == 850)*1\n",
    "y_train = np_utils.to_categorical(y_train, 2)\n",
    "\n",
    "# YIKES https://github.com/ml4a/ml4a-guides/issues/10  !!!!!!!!!!!\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup The Keras Model\n",
    "One note on this part. There was an issue, originally of setting the filter size in this step to anything larger than (1,1).  After Googling around it appears that it had to do with an issue in Keras (described here https://github.com/ml4a/ml4a-guides/issues/10). It seems the order of the image dimensions had a conflict.  With the 2 line fix immediately above this all runs fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# input is 4D tensor with shape: (samples, channels, rows, cols)\n",
    "# output is 4D tensor with shape: (samples, filters, new_rows, new_cols)\n",
    "model.add(Conv2D(32, (3, 3), strides=(1,1), activation='relu', input_shape=(1, imsize, imsize)))\n",
    "\n",
    "model.add(Conv2D(64, (5, 5), strides=(1,1), activation='relu'))\n",
    "\n",
    "#model.add(Conv2D(64, (1, 1), activation='relu'))\n",
    "\n",
    "# MaxPooling2D is a way to reduce the number of parameters in our model by\n",
    "# sliding a 2x2 pooling filter across the previous layer and taking the max of\n",
    "# the 4 values in the 2x2 filter.\n",
    "#model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "\n",
    "# https://www.quora.com/How-does-the-dropout-method-work-in-deep-learning\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "        optimizer='rmsprop', metrics=['accuracy', 'mae', 'mape'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crjones/Documents/Science/HargisDDRF/envs/keras35/lib/python3.5/site-packages/keras/models.py:851: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "13478/13478 [==============================] - 121s - loss: 1.8393 - acc: 0.4925 - mean_absolute_error: 0.5010 - mean_absolute_percentage_error: 250509802.5939   \n",
      "Epoch 2/20\n",
      "13478/13478 [==============================] - 132s - loss: 1.8542 - acc: 0.5050 - mean_absolute_error: 0.5001 - mean_absolute_percentage_error: 250066440.5900   \n",
      "Epoch 3/20\n",
      "13478/13478 [==============================] - 129s - loss: 1.7766 - acc: 0.5089 - mean_absolute_error: 0.4920 - mean_absolute_percentage_error: 246012739.9246   \n",
      "Epoch 4/20\n",
      "13478/13478 [==============================] - 130s - loss: 1.8387 - acc: 0.5176 - mean_absolute_error: 0.4901 - mean_absolute_percentage_error: 245061511.1085   \n",
      "Epoch 5/20\n",
      "13478/13478 [==============================] - 127s - loss: 1.8079 - acc: 0.5220 - mean_absolute_error: 0.4829 - mean_absolute_percentage_error: 241432489.4946   \n",
      "Epoch 6/20\n",
      "13478/13478 [==============================] - 133s - loss: 1.8066 - acc: 0.5399 - mean_absolute_error: 0.4776 - mean_absolute_percentage_error: 238796157.8608   \n",
      "Epoch 7/20\n",
      "13478/13478 [==============================] - 133s - loss: 1.7872 - acc: 0.5407 - mean_absolute_error: 0.4732 - mean_absolute_percentage_error: 236579824.1235   \n",
      "Epoch 8/20\n",
      "13478/13478 [==============================] - 137s - loss: 1.7932 - acc: 0.5503 - mean_absolute_error: 0.4689 - mean_absolute_percentage_error: 234448557.1675   \n",
      "Epoch 9/20\n",
      "13478/13478 [==============================] - 135s - loss: 1.7808 - acc: 0.5671 - mean_absolute_error: 0.4641 - mean_absolute_percentage_error: 232027905.8270   \n",
      "Epoch 10/20\n",
      "13478/13478 [==============================] - 135s - loss: 1.7708 - acc: 0.5863 - mean_absolute_error: 0.4550 - mean_absolute_percentage_error: 227495720.4404   \n",
      "Epoch 11/20\n",
      "13478/13478 [==============================] - 131s - loss: 1.7751 - acc: 0.5973 - mean_absolute_error: 0.4496 - mean_absolute_percentage_error: 224789361.6240   \n",
      "Epoch 12/20\n",
      "13478/13478 [==============================] - 137s - loss: 1.7447 - acc: 0.6140 - mean_absolute_error: 0.4355 - mean_absolute_percentage_error: 217748241.9255   \n",
      "Epoch 13/20\n",
      "13478/13478 [==============================] - 136s - loss: 1.7331 - acc: 0.6343 - mean_absolute_error: 0.4244 - mean_absolute_percentage_error: 212218223.2545   \n",
      "Epoch 14/20\n",
      "13478/13478 [==============================] - 125s - loss: 1.7176 - acc: 0.6459 - mean_absolute_error: 0.4107 - mean_absolute_percentage_error: 205326112.1971   \n",
      "Epoch 15/20\n",
      "13478/13478 [==============================] - 127s - loss: 1.7031 - acc: 0.6637 - mean_absolute_error: 0.3970 - mean_absolute_percentage_error: 198522997.6507   \n",
      "Epoch 16/20\n",
      "13478/13478 [==============================] - 134s - loss: 1.6726 - acc: 0.6847 - mean_absolute_error: 0.3764 - mean_absolute_percentage_error: 188179339.4415   \n",
      "Epoch 17/20\n",
      "13478/13478 [==============================] - 134s - loss: 1.6383 - acc: 0.7106 - mean_absolute_error: 0.3530 - mean_absolute_percentage_error: 176483954.2389   \n",
      "Epoch 18/20\n",
      "13478/13478 [==============================] - 135s - loss: 1.6014 - acc: 0.7386 - mean_absolute_error: 0.3243 - mean_absolute_percentage_error: 162166244.8969   \n",
      "Epoch 19/20\n",
      "13478/13478 [==============================] - 134s - loss: 1.5546 - acc: 0.7629 - mean_absolute_error: 0.2945 - mean_absolute_percentage_error: 147261526.9541   \n",
      "Epoch 20/20\n",
      "13478/13478 [==============================] - 136s - loss: 1.5204 - acc: 0.7842 - mean_absolute_error: 0.2650 - mean_absolute_percentage_error: 132514450.0395   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1246d2cc0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, nb_epoch=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the testing data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create test inds for 750\n",
    "possible_test_inds_750 = list(set(range(len(subimgs['750']))) - set(training_random_inds_750))\n",
    "\n",
    "N_testing = max(3000, int(0.2*len(possible_test_inds_750)))\n",
    "\n",
    "test_random_inds_750 = np.random.choice(possible_test_inds_750, N_testing//2)\n",
    "X_test = [subimgs['750'][x]  for x in test_random_inds_750]\n",
    "y_test = [750]*(N_testing//2)\n",
    "\n",
    "# Extend the X_test and y_test with the '850' data\n",
    "possible_test_inds_850 = list(set(range(len(subimgs['850']))) - set(training_random_inds_850))\n",
    "test_random_inds_850 = np.random.choice(possible_test_inds_850, N_testing//2)\n",
    "X_test.extend([subimgs['850'][x]  for x in test_random_inds_850])\n",
    "y_test.extend([850]*(N_testing//2))\n",
    "\n",
    "# Randomize the list of training data\n",
    "inds = [x for x in range(len(X_test))]\n",
    "random.shuffle(inds)\n",
    "X_test = [X_test[ii] for ii in inds]\n",
    "y_test = [y_test[ii] for ii in inds]\n",
    "\n",
    "# Now make them into a 3D array and 1D array\n",
    "X_test = np.stack(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Add dimension to X_train for keras\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, imsize, imsize)\n",
    "\n",
    "# change y_test to labels of 0 and 1 (conversion of boolean to int)\n",
    "y_test = np.array(y_test == 850)*1\n",
    "y_test = np_utils.to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 12s    \n",
      "loss=2.6011208941141764, acc=0.5303333333333333, mean_absolute_error=0.47993534231185914, mean_absolute_percentage_error=239967693.696\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "names = model.metrics_names\n",
    "print('{}={}, {}={}, {}={}, {}={}'.format(names[0], scores[0], names[1], scores[1], names[2], scores[2], names[3], scores[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
